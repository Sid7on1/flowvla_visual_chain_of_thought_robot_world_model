{
  "agent_id": "coder2",
  "task_id": "task_9",
  "files": [
    {
      "filename": "scripts/precompute_flow.py",
      "purpose": "Script to precompute optical flow fields using RAFT for all training videos. Saves flow fields as numpy arrays for efficient loading during training.",
      "priority": "medium",
      "dependencies": [
        "torch",
        "opencv-python",
        "numpy",
        "tqdm",
        "raft_torch",
        "datasets"
      ],
      "key_functions": [
        "precompute_all_flows",
        "compute_flow_for_video",
        "save_flow_batch",
        "validate_flow_quality"
      ],
      "estimated_lines": 200,
      "complexity": "medium"
    },
    {
      "filename": "requirements.txt",
      "purpose": "Complete list of Python dependencies with specific versions for reproducible installation.",
      "priority": "low",
      "dependencies": [],
      "key_functions": [],
      "estimated_lines": 50,
      "complexity": "low"
    }
  ],
  "project_info": {
    "project_name": "FlowVLA_Visual_Chain_of_Thought_Robot_World_Model",
    "project_type": "transformer",
    "description": "FlowVLA is a Vision-Language-Action (VLA) model that introduces Visual Chain of Thought (Visual CoT) for learning physically-grounded world models. Instead of direct next-frame prediction, it decomposes the process into a structured reasoning chain: first predicting optical flow (motion dynamics) and then using this flow to predict the next frame. This vt\u2192ft\u2192vt+1 approach enables disentangled learning of appearance and motion, leading to more coherent visual predictions and efficient policy learning for robotics manipulation tasks.",
    "key_algorithms": [
      "Visual_Chain_of_Thought_Reasoning",
      "Optical_Flow_to_RGB_Encoding",
      "Autoregressive_Transformer_Next_Token_Prediction",
      "VQ_GAN_Tokenization",
      "RAFT_Optical_Flow_Precomputation",
      "FAST_Action_Tokenization"
    ],
    "main_libraries": [
      "torch",
      "transformers",
      "opencv-python",
      "numpy",
      "pillow",
      "tqdm",
      "wandb",
      "hydra-core",
      "einops",
      "accelerate",
      "datasets",
      "tokenizers",
      "raft_torch",
      "scipy",
      "matplotlib",
      "seaborn"
    ]
  },
  "paper_content": "PDF: cs.RO_2508.18269v1_FlowVLA-Thinking-in-Motion-with-a-Visual-Chain-of.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nFlowVLA: Thinking in Motion with a Visual Chain\nof Thought\nZhide Zhong1, Haodong Yan1, Junfeng Li1, Xiangchen Liu1, Xin Gong1, Wenxuan Song1, Jiayi\nChen1, and Haoang Li1\n1HKUST(GZ)\nMany Vision-Language-Action (VLA) models rely on an internal world model trained via next-frame prediction.\nThis approach, however, struggles with physical reasoning as it entangles static appearance with dynamic\nmotion, often resulting in implausible visual forecasts and inefficient policy learning. To address these\nlimitations, we introduce the Visual Chain of Thought (Visual CoT): a pre-training framework that encourages\na model to reason about how a scene evolves before predicting what it will look like. We instantiate this\nprinciple in FlowVLA, which predicts a future frame ( vt+1) only after generating an intermediate optical flow\nrepresentation ( ft) that encodes motion dynamics. This \u201c vt\u2192ft\u2192vt+1\u201d reasoning process is implemented\nwithin a single autoregressive Transformer, guiding the model to learn disentangled dynamics. As a result,\nFlowVLA produces coherent visual predictions and facilitates more efficient policy learning. Experiments on\nchallenging robotics manipulation benchmarks demonstrate state-of-the-art performance with substantially\nimproved sample efficiency, pointing toward a more principled foundation for world modeling. Project page:\nhttps://irpn-lab.github.io/FlowVLA/\nKeywords: Visual Chain of Thought, World Models, Vision-Language-Action Models\n1. Introduction\nRecent advances in Vision-Language-Action (VLA) models Kim et al. (2024), Zitkovich et al. (2023), Black\net al. (2024), Team et al. (2024), particularly those pre-trained as world models like UniVLA Wang et al.\n(2025) and WorldVLA Cen et al. (2025), have shown remarkable promise for creating generalist robots. The\nprevailing strategy involves training a large autoregressive transformer to predict the next visual frame given\npast observations, effectively learning the dynamics of the environment from vast amounts of video data.\nThis learned world model then serves as a powerful foundation for fine-tuning downstream action policies.\nDespite their success, these models suffer from a critical, foundational flaw: they conflate the task of physical\nreasoning with simple pixel prediction. This next-frame prediction paradigm is often a \u201cpixel-copying trap\u201d,\nwhere the model learns to replicate static backgrounds without a deep understanding of spatiotemporal\ndynamics, leadingtoblurry, inconsistent, andphysicallyimplausiblelong-horizonforecastsMingetal.(2024).\nFurthermore, this approach creates a significant domain gap between the passive, observational knowledge\nlearned during pre-training and the active, control-oriented knowledge required for policy learning. This\nresults in inefficient knowledge transfer and requires extensive fine-tuning, as evidenced by slow convergence\non downstream tasks Zeng et al. (2024).\nWe argue that the key to learning physically-grounded dynamics is to force the model to reason explicitly.\nDrawing inspiration from the success of Chain of Thought (CoT) prompting in Large Language Models Wei\net al. (2022), which enhances reasoning by generating intermediate steps, we propose a novel counterpart\nfor world models: a Visual Chain of Thought (Visual CoT) . Instead of a single, opaque leap from thearXiv:2508.18269v1  [cs.RO]  25 Aug 2025\n\n--- Page 2 ---\nFlowVLA: Thinking in Motion with a Visual Chain of Thought\nStage 1: World Model Pre-trainingStage 2: Policy Fine-tuningFrame TExplicit Mo+on Reasoning \nFrame T+k\nFrame T+2k\nFrame TFrame T+kFrame T+2k\nAc+on Strategy\nPhysically Plausible & Motion Coherence \n!\nE\ufb03cient Adap+on& High Accuracy \n\u26a1Flow TFlow T+kFlow T+2k\nAc0onTAc0on T+kAc0on T +2kFlowVLA\nFigure 1: The FlowVLA Two-Stage Training Paradigm. (Top) Stage 1: World Model Pre-training with Visual\nCoT.The model learns to predict an intermediate motion representation (Flow T) from an initial frame (Frame T),\nand then uses both to forecast the subsequent frame (Frame T+k). This iterative process yields physically plausible,\nlong-horizon video predictions. (Bottom) Stage 2: Policy Fine-tuning. The pre-trained world model is adapted for\ncontrol, fine-tuned to generate precise robot actions (Action T) from visual observations. This paradigm leverages the\nlearned dynamics for efficient and accurate policy learning.\ncurrent frame vtto the next vt+1, we decompose the prediction into a structured reasoning process: first,\npredict the intermediate physical dynamics\u2014the optical flow ftthat describes howevery pixel will move.\nOnly then, conditioned on this explicit motion plan, predict the resulting future frame. This vt\u2192ft\u2192vt+1\ncausal chain (see Figure 1 for an overview) transforms the learning objective from mere pattern recognition\ninto a structured physical reasoning task.\nWe introduce FlowVLA as the first concrete realization of this principle. A key aspect of our design is to\nintegrate motion without introducing dedicated architectural components. We encode optical flow fields\nas standard RGB-like images, allowing them to be processed by the exact same VQ tokenizer as regular\ncamera observations. This enables a single, unmodified autoregressive transformer to seamlessly learn the\ninterleaved sequence of appearance and motion tokens. This design makes FlowVLA a truly Unified Visual\nCoT, where the reasoning steps (flow) and states (frames) are expressed in a shared vocabulary and processed\nby a single, unified model.\nOur work makes the following contributions:\n\u2022We identify a fundamental limitation of next-frame prediction for VLA world models and propose\nVisual Chain of Thought (Visual CoT) as a new principle for learning dynamics.\n\u2022WeintroduceFlowVLA,asimpleyeteffectiveinstantiationthatunifiesappearanceandmotionreasoning\n2\n\n--- Page 3 ---\nFlowVLA: Thinking in Motion with a Visual Chain of Thought\nPolicy Fine-tuningText TokenizerShared ImageTokenizerImageDe-Tokenizer\nText TokenizerShared ImageTokenizerActionDe-Tokenizer\n\u00d7N\nWorld Model Pre-training\u00d7MImageDe-Tokenizer\u00d7N\u00d7K\nFigure 2: The FlowVLA Framework Architecture. Our model follows a two-stage paradigm. (Left) Stage 1: World\nModel Pre-training with Visual CoT. Input frames are encoded into appearance tokens (orange). The model then\nautoregressively predicts an interleaved sequence of motion tokens (cyan, representing flow) and future appearance\ntokens. This structured vt\u2192ft\u2192vt+1prediction forces the model to reason about dynamics before forecasting the\nfuture.(Right) Stage 2: Policy Fine-tuning. The pre-trained world model is adapted for control. Conditioned on a\ntext instruction (green) and the current observation (orange), the model autoregressively predicts action tokens (blue)\nthat are decoded into robot commands.\nwithin a single autoregressive Transformer via shared tokenization.\n\u2022We demonstrate through extensive experiments that FlowVLA achieves state-of-the-art performance on\nchallenging manipulation benchmarks, while offering superior sample efficiency, supporting our claim\nthat explicit motion reasoning better bridges the gap between pre-training and policy fine-tuning.\n2. FlowVLA\nIn this section, we introduce FlowVLA, a novel framework designed to instantiate our proposed Visual Chain\nof Thought (Visual CoT) principle for world model pre-training. We first provide a high-level overview of our\ntwo-stage training paradigm. We then detail the core of our contribution: the Visual CoT pre-training stage,\nincluding our unified tokenization scheme for appearance and motion. Finally, we describe how the learned\nworld model is finetuned for downstream robotics tasks.\n2.1. Framework Overview\nFlowVLA follows a two-stage training paradigm, consistent with state-of-the-art methods like UniVLA Wang\net al. (2025) and WorldVLA Cen et al. (2025) to ensure a fair basis for comparison.\n3\n\n--- Page 4 ---\nFlowVLA: Thinking in Motion with a Visual Chain of Thought\n1.Stage 1: World Model Pre-training: The model learns general physical dynamics from large-scale,\naction-free video data by executing our proposed Visual Chain of Thought.\n2.Stage 2: Policy Finetuning: The pre-trained model weights are finetuned on downstream, action-\nannotated robotics datasets to learn specific control policies.\nThe primary contribution of this paper is concentrated in Stage 1, where we redefine the objective of world\nmodel pre-training from simple next-frame prediction to structured physical reasoning.\n2.2. Stage 1: World Model Pre-training via Visual Chain of Thought\nThe goal of this stage is to learn a robust world model by compelling it to reason about dynamics before\npredicting future states. This is achieved through our Visual Chain of Thought (Visual CoT) pre-training\ntask. Below, we detail the tokenization scheme that unifies appearance and motion, and then describe the\nautoregressive objective used to learn the reasoning chain.\nUnified Motion and Appearance Tokenization A key challenge is to represent two distinct physical\nsignals, appearance (images) and motion (optical flow), within a single model. Our solution is a unified\ntokenization scheme that preserves architectural simplicity. To process both appearance and motion, we\nfirst represent the 2-channel optical flow fields as standard RGB images. Following the technique from\nVideoJAM Chefer et al. (2025), for each pixel\u2019s flow vector (u,v), we convert it to a color representation\nbased on its polar coordinates. The direction of motion is mapped to the color\u2019s Hue, calculated from the\nangle \u03b1=arctan 2(v,u). The speed of motion is mapped to the color\u2019s Saturation and Value (Brightness),\nderived from the vector\u2019s magnitude m=\u221a\nu2+v2. To ensure that subtle movements are not lost while large\nmotions do not saturate the representation, the magnitude is normalized to the range [0, 1]using a scaling\ncoefficient \u03c3=0.15:\nmnorm=min(1.0,m\n\u03c3\u22c5\u221a\nH2+W2), (1)\nwhere HandWare the height and width of the frame.\nCrucially, these resulting flow images, along with the original RGB frames, are processed by the exact\nsame pre-trained VQ-GAN tokenizer Esser et al. (2021). This approach discretizes both modalities into\ntoken sequences from a shared vocabulary, offering three key advantages: 1) Parameter Efficiency , as no\nnew motion-specific tokenizer is needed; 2) Architectural Simplicity , maintaining a single, end-to-end\nautoregressive pipeline; and 3) Unified Representation , enabling the model to learn correlations between\nappearance and motion in a shared latent space.\nAutoregressive Learning of the Visual CoT With a unified token representation for both frames ( vt) and\nflow ( ft), we construct a reasoning chain vt\u2192ft\u2192vt+1. We employ a standard decoder-only Transformer,\ntraining it to predict an interleaved sequence of frames and optical flow fields given an optional language\ninstruction Linstr:\nSwm={Linstr,v0,f0,v1,f1, . . . , vT,fT} (2)\nThe model is trained using a standard next-token prediction objective, maximizing the log-likelihood of the\nsequence. The loss of the world model, \u2112WM, is the sum of the cross-entropy losses in both the reasoning step\n(flow tokens) and the final state (next frame tokens). Formally, for each timestep t, the model first predicts\n4\n\n--- Page 5 ---\nFlowVLA: Thinking in Motion with a Visual Chain of Thought\nthe flow ftbased on all preceding tokens, and then predicts the next frame vt+1conditioned on both the\nhistory and the just-predicted flow:\n\u2112WM=T\u22121\n\u2211\nt=0(\u2112CE(ft\u2223S<vt+1)+\u03bb\u22c5\u2112CE(vt+1\u2223S<vt+1,ft)) (3)\nwhere S<vt+1denotes all the tokens preceding vt+1, and \u03bbis a balancing hyperparameter (set to 1.0 in our\nexperiments). This objective explicitly forces the model to perform a \u201creason \u2192predict\u201d process during both\ntraining and inference.\n2.3. Stage 2: Finetuning for Robotic Control\nInitialization and Task. The policy model is initialized with the weights from the pre-trained FlowVLA.\nDuring this stage, the input sequence is composed of interleaved observations and actions: Spolicy=\n{Linstr,v0,a0,v1,a1, . . .}, where atrepresents the robot\u2019s action tokens.\nAction Tokenization and Objective. Actions are discretized into tokens following the FAST Pertsch et al.\n(2025). Critically, the fine-tuning loss, \u2112policy, is computed onlyover the action tokens. This objective directs\nthe model to leverage all its learned visual and dynamical knowledge towards the singular goal of making\ncorrect action decisions.\n3. Experiments\nWe conduct a comprehensive set of experiments to validate the effectiveness of our proposed Visual Chain of\nThought framework. Our evaluation is designed to answer four key questions:\nQ1:Does FlowVLA achieve state-of-the-art performance on complex, long-horizon robotics tasks?\nQ2:Does explicit motion reasoning lead to superior world modeling capabilities?\nQ3:Is FlowVLA more sample-efficient during policy finetuning, validating our claim of bridging the pre-\ntraining/finetuning gap?\nQ4:Which components of our design are most critical to its success?\n3.1. Experimental Setup\nBenchmarks. To comprehensively evaluate FlowVLA\u2019s generalization capabilities, we test our method on\ntwo distinct and challenging benchmarks: LIBERO and SimplerEnv.\nLIBERO Liu et al. (2023) serves as our primary benchmark for evaluating generalization across multiple\naxes. We follow the standard behavioral cloning setup and report performance on its four main suites, which\ntest generalization to novel spatial layouts, objects, task goals, and long-horizon compositional challenges.\nSimplerEnv Li et al. (2024) is used to assess the robustness of our model against significant domain shifts.\nThis benchmark is specifically designed to evaluate policy transfer by introducing diverse variations in\nlighting, textures, and camera viewpoints, which are more representative of real-world complexity.\nImplementation Details. Our FlowVLA model is built on the 8.5B parameter Emu3 Wang et al. (2024)\nand UniVLA Wang et al. (2025) architecture. Our key change is adding optical flow, pre-computed with\n5\n\n--- Page 6 ---\nFlowVLA: Thinking in Motion with a Visual Chain of Thought\nRAFT Teed and Deng (2020), as an additional input to represent motion. We follow the standard training\nsetup for each benchmark: the model for LIBERO is trained only on the LIBERO dataset, while the model for\nSimplerEnv is trained only on the Bridge V2 dataset Walke et al. (2023). For LIBERO, we pre-train the world\nmodel for 5k steps with a batch size of 16, and then fine-tune the policy for 5k steps with a batch size of 96.\nFor the SimplerEnv benchmark, pre-training runs for 12k steps with a batch size of 32 and policy fine-tuning\nfor 20k steps with a batch size of 128.\n3.2. Main Results on Robotics Benchmarks (Q1)\nTo answer Q1, we evaluate the final performance of FlowVLA after policy finetuning on two distinct and\nchallenging benchmarks: LIBERO and SimplerEnv. Our method establishes a new state-of-the-art on both,\ndemonstrating its effectiveness and robustness.\nResults on LIBERO. As shown in Table 1, FlowVLA consistently outperforms all prior methods across the four\nevaluated suites. Notably, the performance gains are most significant on the Longhorizon tasks. This directly\nhighlights the benefit of learning a world model with a more robust understanding of physical dynamics, as\nour Visual CoT framework enables better long-term planning and reasoning.\nResults on SimplerEnv. We further test our model\u2019s robustness on the SimplerEnv benchmark, which\nintroduces significant visual domain shifts. Table 2 shows that FlowVLA achieves a substantial improvement\nover existing methods. The remarkable success on tasks that were previously challenging for other models\n(e.g., stacking blocks) validates that our explicit motion reasoning leads to policies that are more resilient to\nthe visual and physical variations found in more realistic environments.\n3.3. Analysis of World Modeling Capabilities (Q2)\nTodemonstratethesuperiorityofourVisualChainofThought(VisualCoT)framework, weconductadetailed\nqualitative analysis on the challenging, real-world Bridge V2 dataset. Our investigation reveals two distinct\nand critical failure modes in the standard next-frame prediction baseline.\nFirst, Figure 3 highlights failures in physical plausibility. In these examples, the baseline model generates\nphysically incoherent rollouts, such as causing the robotic arm to suddenly vanish or producing inconsistent\nobject motion. This indicates a fundamental inability to model the basic physical continuity of a scene.\nSecond, Figure 4 illustrates a more subtle but equally critical issue: semantic inconsistency. Here, while\nthe predicted frames from the baseline may appear visually coherent, the depicted action fails to follow the\ngiven language command. This reveals a disconnect between language understanding and visual forecasting.\nIn stark contrast, FlowVLA successfully navigates both challenges across all scenarios. By first reasoning\nabout motion dynamics via optical flow, our model generates predictions that are not only physically plausible\nbut also semantically aligned with the task instructions, showcasing the robustness and generalizability of\nour approach.\n3.4. Convergence Speed and Data Efficiency(Q3)\nFigure 5 illustrates FlowVLA\u2019s dramatic advantage in training and sample efficiency. In the full-data setting\n(Figure 5a), FlowVLA proves vastly more sample-efficient, reaching the baseline\u2019s peak performance (0.64)\nwith only one-third of the training steps (2k vs. 6k) while also achieving a higher final success rate of 0.73.\nThis efficiency advantage is amplified in the more challenging low-data regime (Figure 5b). Here, the\n6\n\n--- Page 7 ---\nFlowVLA: Thinking in Motion with a Visual Chain of Thought\nTable 1: Results on the LIBERO Benchmark. We report the final task success rate (%). We compare FlowVLA against\nstate-of-the-art methods, grouped by their core methodology. Our key comparison is within the w/ World Model\ngroup, where our Visual CoT pre-training demonstrates superior performance. Notably, FlowVLA achieves this without\nrelying on massive external datasets , highlighting the efficiency of our proposed framework.\nModelLarge Scale\nPretrainSpatial Object Goal Long Avg.\nw/o World Model\nDiffusion Policy Chi et al. (2023) \u00d7 78.3 92.5 68.3 50.5 72.4\nOcto Team et al. (2024) \u2713 78.9 85.7 84.6 51.1 75.1\nOpenVLA Kim et al. (2024) \u2713 84.7 88.4 79.2 53.7 76.5\nDiT Policy Hou et al. (2025) \u2713 84.2 96.3 85.4 63.8 82.4\nTraceVLA Zheng et al. (2024) \u2713 84.6 85.2 75.1 54.1 74.8\nSpatialVLA Qu et al. (2025) \u2713 88.2 89.9 78.6 55.5 78.1\npi0-FAST Pertsch et al. (2025) \u2713 96.4 96.8 88.6 60.2 85.5\nThinkAct Huang et al. (2025a) \u2713 88.3 91.4 87.1 70.9 84.4\nw/ World Model\nWorldVLA Cen et al. (2025) \u00d7 85.6 89.0 82.6 59.0 79.1\nUniVLA\u2020Wang et al. (2025) \u00d7 92.6 93.8 86.6 63.0 84.0\nCoT-VLA Zhao et al. (2025) \u2713 87.5 91.6 87.6 69.0 81.1\nFlowVLA (ours) \u00d7 93.2 95.0 91.6 72.6 88.1\n\u2020Our reported UniVLA result is from our re-implementation, pre-trained only on LIBERO without wrist camera images for a fair\ncomparison. The original paper utilized large-scale robotics data and an additional wrist camera.\nTable 2: Results on the SimplerEnv-WidowX benchmark. We report the final task success rate (%). FlowVLA\nsignificantly surpasses prior methods, demonstrating superior robustness to the visual domain shifts present in this\nbenchmark. Best results are in bold.\nModel Put Spoon Put Carrot Stack Block Put Eggplant Avg.\nRT-1-X Team et al. (2024) 0.0 4.2 0.0 0.0 1.1\nOcto-Base Team et al. (2024) 12.5 8.3 0.0 43.1 16.0\nOcto-Small Team et al. (2024) 47.2 9.7 4.2 56.9 29.5\nOpenVLA Team et al. (2024) 0.0 0.0 0.0 4.1 1.0\nRoboVLMs Liu et al. (2025) 45.8 20.8 4.2 79.2 37.5\nSpatialVLA Qu et al. (2025) 16.7 25.0 29.2 100 42.7\nRoboPoint Yuan et al. (2024) 16.7 20.8 8.3 25.0 17.7\nFSD Yuan et al. (2025a) 41.6 50.0 33.3 37.5 40.6\nEmbodied-R1 Yuan et al. (2025b) 62.5 68.0 36.1 58.3 56.2\nUniVLA\u2020Wang et al. (2025) 62.5 62.5 41.6 95.8 65.6\nFlowVLA (Ours) 70.8 62.5 62.5 100.0 74.0\n\u2020Result obtained by evaluating the officially released checkpoint.\nperformance gap widens substantially. FlowVLA not only achieves a 55% higher peak success rate relative\nto the baseline (0.48 vs. 0.31) but also surpasses the baseline\u2019s peak performance in just 1,000 steps. This\n7\n\n--- Page 8 ---\nFlowVLA: Thinking in Motion with a Visual Chain of Thought\n(a)Task: \"Put the rectangular on top of the rectangular block next to it.\"\n(b)Task: \"Move the spoon so that it sits to the left of the metal pot.\"\nFigure 3: Analysis of Physical Plausibility on the Bridge V2 Benchmark. This figure highlights common physical\nfailures in the baseline model. In both examples, the baseline model (middle row) struggles to maintain physical\nconsistency, leading to implausible outcomes such as a disappearing manipulator or erratic object behavior. In contrast,\nFlowVLA (bottom row), guided by its motion-first reasoning, produces stable and physically coherent predictions that\naccurately reflect the scene\u2019s dynamics.\nsubstantial improvement in sample efficiency validates our core hypothesis: by requiring the model to\n8\n\n--- Page 9 ---\nFlowVLA: Thinking in Motion with a Visual Chain of Thought\n(a)Task: \"Put the toy into left of table.\"\n(b)Task: \"Move toy diagonally little bit top on the right side.\"\nFigure 4: Analysis of Semantic Alignment on the Bridge V2 Benchmark. This figure illustrates the baseline\u2019s failure\nto align predictions with language instructions. While the predicted frames from baseline model (middle row) might\nappear visually plausible at a glance, the resulting motion does not correspond to the specified task (e.g., moving an\nobject in the wrong direction). FlowVLA (bottom row) again demonstrates superior performance, correctly interpreting\nthe command and generating a corresponding visual trajectory. This underscores that our Visual CoT not only improves\nphysical realism but also enhances the model\u2019s ability to ground language in action.\n9\n\n--- Page 10 ---\nFlowVLA: Thinking in Motion with a Visual Chain of Thought\n(a)Training on 100% of data\n (b)Training on 50% of data\nFigure 5: Training Efficiency Comparison in Full and Low-Data Regimes. Success rate versus training steps for\nFlowVLA and the baseline. Our method converges dramatically faster and reaches a higher peak performance across\nboth the full dataset (a)and a data-scarce setting (b). The performance gap widens significantly with limited data,\nhighlighting the superior sample efficiency of our approach.\nexplicitly reason about motion via a visual chain-of-thought, FlowVLA benefits from a powerful inductive\nbias. This simplifies the learning of physical dynamics from raw pixels, leading to a more effective and robust\nlearning process, particularly when data is limited.\n3.5. Ablation Studies (Q4)\nFinally, we conduct a series of ablation studies to understand the contribution of each key component in our\nframework. The results, summarized in Table 3, are evaluated on the LIBERO-10 benchmark.\nWe first remove the entire Visual Chain-of-Thought (CoT) structure, which causes our model to degenerate\ninto the UniVLA baseline. As shown in Table 3, the success rate drops sharply from 73.0% to 64.0%. This\nsignificant 9-point drop confirms that the explicit, step-by-step reasoning process, where the model first\nthinks about \"how to move\" before predicting the outcome, is the primary driver of our model\u2019s performance\ngain.\nNext, we investigate the importance of direct supervision for the intermediate reasoning step. In this variant,\nwe retain the interleaved visual-flow sequence structure but remove the optical flow loss during training,\nmeaning the model is not explicitly guided to generate physically correct flows. The performance degrades to\n69.5%. This result indicates that while the interleaved architecture provides a useful structural prior, direct\nsupervision is crucial to prevent the model from generating arbitrary or collapsed representations for the\nintermediate step ( ft). The supervision ensures the \u201cthought\u201d is physically grounded.\nFinally, we challenge the core design of interleaving information. We restructure the input sequence by\ngrouping all visual frames first, followed by all corresponding flow frames (i.e., v0,v1, ...,f0,f1, ...). This\nconfiguration leads to a severe performance collapse, with the success rate plummeting to 49.4%. This\nis because the model can no longer leverage the predicted motion ftto inform the generation of the next\nstate vt+1in a causal, forward-looking manner. This result provides strong evidence that the \u201cinterleaved,\nstep-by-step causal chain\u201d ( vt\u2192ft\u2192vt+1) is essential for effective planning and action generation.\n10\n\n--- Page 11 ---\nFlowVLA: Thinking in Motion with a Visual Chain of Thought\nTable 3: Ablation studies on the LIBERO-10 benchmark. We evaluate the importance of our key design choices: the\nVisual CoT structure, the flow supervision loss, and the interleaved sequence format. The full FlowVLA model is shown\nfor comparison.\nConfiguration Success Rate (%)\nFlowVLA (Ours, Full Model) 73.0\nAblations:\n1. w/o CoT (degenerates to baseline) 64.0\n2. w/o Flow Loss 69.5\n3. Grouped Sequence 49.4\n4. Related Work\n4.1. Vision-Language-Action (VLA) Models\nThedominantparadigmforcreatinggeneralistrobotagentsistheVision-Language-Action(VLA)modelZitkovich\net al. (2023), Kim et al. (2024), Black et al. (2024), ?),?. These models extend large, pre-trained Vision-\nLanguage Models (VLMs) by fine-tuning them on extensive robotics datasets O\u2019Neill et al. (2024). Architec-\ntures like RT-2 Zitkovich et al. (2023) and OpenVLA Kim et al. (2024) treat action generation as a sequence\nmodeling problem, directly mapping visual and textual inputs to discretized action tokens. Other recent\nworks have focused on improving the action representation itself, using techniques like diffusion policies Chi\net al. (2023) or flow matching Black et al. (2024). While this end-to-end approach has demonstrated\nremarkable generalization, it often treats the environment\u2019s physical dynamics as a \u201cblack box\u201d. The policy is\nlearned reactively, without an explicit, underlying model of how the world functions or evolves. FlowVLA\ndiverges from this standard VLA formulation by prioritizing world understanding over immediate action\ngeneration. Its pre-training objective is not to learn a policy ( vt\u2192at), but to build a robust world model\nby learning the physical transition function of the environment ( vt\u2192vt+1). This \u201cdynamics-first\u201d approach\nestablishes a solid foundation of physical knowledge before it is adapted for downstream control.\n4.2. World Models for Robotics\nTheconceptofaworldmodel, whichlearnsamodelofitsenvironmenttoplanorimaginefutureoutcomesHa\nand Schmidhuber (2018), is increasingly vital in robotics. Recent works have leveraged this idea for policy\nlearning. For example, some models use video prediction as a form of self-supervised pre-training to improve\ndownstream task performance Wang et al. (2025), Wu et al. (2023). Others, like WorldVLA Cen et al. (2025),\npropose architectures that jointly learn to predict both the next frame and the next action, creating a tight\nloop between prediction and control. A common thread in these approaches is the direct prediction of the\nnext frame, modeling the transition as vt\u2192vt+1. However, this direct objective forces a single network to\nsimultaneously handle two distinct problems: understanding static scene properties (appearance, texture,\nlighting) and modeling complex physical dynamics (motion, interaction, causality). This entanglement can\nresult in inefficient learning and physically implausible predictions, such as blurry or distorted futures. In\ncontrast, FlowVLA avoids this entanglement with its Visual Chain of Thought framework. We decompose\nthe prediction into a \u201cframe \u2192flow\u2192frame\u201d reasoning process. By forcing the model to first predict an\nintermediate optical flow field ( ft), we explicitly decouple the learning of dynamics ( howthings move) from\nappearance ( whatthey look like), resulting in a more causally-grounded world model.\n11\n\n--- Page 12 ---\nFlowVLA: Thinking in Motion with a Visual Chain of Thought\n4.3. Embodied Reasoning for Robotics\nTo move beyond simple reactive policies, a significant line of research has focused on endowing agents with\nexplicit reasoning capabilities. These approaches can be broadly categorized. One category focuses on high-\nlevel semantic reasoning, where models generate linguistic or abstract plans. For instance, ECoT Zawalski\net al. (2024) and ThinkAct Huang et al. (2025b) leverage Chain-of-Thought prompting to generate textual\nsub-goals that guide the agent\u2019s behavior. A second category focuses on mid-level geometric reasoning,\nwhere models produce intermediate spatial representations to guide actions. MolmoAct Lee et al. (2025),\nfor example, generates depth maps and 2D end-effector trajectory traces as part of its \u201cAction Reasoning\u201d\npipeline to make planning more concrete. FlowVLA introduces a more fundamental form of reasoning:\nlow-level physical reasoning. Unlike high-level semantic or geometric planning, our Visual CoT operates at\nthe pixel level. By predicting the dense optical flow field, it learns a general, causal model of the world\u2019s\ndynamics, independent of any specific task or action. This provides a foundational understanding of physics\nthat is complementary to, and arguably a prerequisite for, effective high-level control.\n5. Conclusion\nWe proposed the Visual Chain of Thought (Visual CoT) as a new principle for world model learning,\ninstantiated in FlowVLA. By decomposing prediction into a motion\u2013then\u2013appearance process, FlowVLA\nlearns more coherent dynamics within a unified autoregressive model. Experiments on robotics benchmarks\ndemonstrate state-of-the-art performance and improved sample efficiency, highlighting the value of explicit\nmotion reasoning for bridging perception and control.\n12\n\n--- Page 13 ---\nFlowVLA: Thinking in Motion with a Visual Chain of Thought\nReferences\nKevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy\nGroom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Sergey Levine, Adrian\nLi-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Lucy Xiaoyang Shi, James Tanner, Quan Vuong, Anna\nWalling, Haohuan Wang, and Ury Zhilinsky. \u03c00: A vision-language-action flow model for general robot\ncontrol, 2024. URL https://arxiv.org/abs/2410.24164 .\nJun Cen, Chaohui Yu, Hangjie Yuan, Yuming Jiang, Siteng Huang, Jiayan Guo, Xin Li, Yibing Song, Hao Luo,\nFan Wang, et al. Worldvla: Towards autoregressive action world model. arXiv preprint arXiv:2506.21539 ,\n2025.\nHila Chefer, Uriel Singer, Amit Zohar, Yuval Kirstain, Adam Polyak, Yaniv Taigman, Lior Wolf, and Shelly\nSheynin. VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion Generation in Video\nModels. arXiv preprint arXiv:2502.02492 , 2025. URL https://arxiv.org/abs/2502.02492 .\nCheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and\nShuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. The International Journal\nof Robotics Research , page 02783649241273668, 2023.\nPatrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis.\nInProceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 12873\u201312883,\n2021.\nDavid Ha and J\u00fcrgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122 , 2(3), 2018.\nZhi Hou, Tianyi Zhang, Yuwen Xiong, Haonan Duan, Hengjun Pu, Ronglei Tong, Chengyang Zhao, Xizhou\nZhu, Yu Qiao, Jifeng Dai, et al. Dita: Scaling diffusion transformer for generalist vision-language-action\npolicy. arXiv preprint arXiv:2503.19757 , 2025.\nChi-Pin Huang, Yueh-Hua Wu, Min-Hung Chen, Yu-Chiang Frank Wang, and Fu-En Yang. Thinkact: Vision-\nlanguage-action reasoning via reinforced visual latent planning. arXiv preprint arXiv:2507.16815 , 2025a.\nChi-Pin Huang, Yueh-Hua Wu, Min-Hung Chen, Yu-Chiang Frank Wang, and Fu-En Yang. Thinkact: Vision-\nlanguage-action reasoning via reinforced visual latent planning, 2025b. URL https://arxiv.org/abs/\n2507.16815 .\nMoo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov,\nEthan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model.\narXiv preprint arXiv:2406.09246 , 2024.\nJason Lee, Jiafei Duan, Haoquan Fang, Yuquan Deng, Shuo Liu, Boyang Li, Bohan Fang, Jieyu Zhang, Yi Ru\nWang, Sangho Lee, Winson Han, Wilbert Pumacay, Angelica Wu, Rose Hendrix, Karen Farley, Eli VanderBilt,\nAli Farhadi, Dieter Fox, and Ranjay Krishna. Molmoact: Action reasoning models that can reason in space,\n2025. URL https://arxiv.org/abs/2508.07917 .\nXuanlin Li, Kyle Hsu, Jiayuan Gu, Karl Pertsch, Oier Mees, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat,\nIsabel Sieh, Sean Kirmani, et al. Evaluating real-world robot manipulation policies in simulation. arXiv\npreprint arXiv:2405.05941 , 2024.\n13\n\n--- Page 14 ---\nFlowVLA: Thinking in Motion with a Visual Chain of Thought\nBo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone. Libero: Benchmarking\nknowledge transfer for lifelong robot learning. Advances in Neural Information Processing Systems , 36:\n44776\u201344791, 2023.\nHuaping Liu, Xinghang Li, Peiyan Li, Minghuan Liu, Dong Wang, Jirong Liu, Bingyi Kang, Xiao Ma, Tao Kong,\nand Hanbo Zhang. Towards generalist robot policies: What matters in building vision-language-action\nmodels. 2025.\nRuibo Ming, Zhewei Huang, Zhuoxuan Ju, Jianming Hu, Lihui Peng, and Shuchang Zhou. A survey on video\nprediction: From deterministic to generative approaches. CoRR, 2024.\nAbby O\u2019Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn\nPooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, et al. Open x-embodiment: Robotic learning datasets\nand rt-x models: Open x-embodiment collaboration 0. In 2024 IEEE International Conference on Robotics\nand Automation (ICRA) , pages 6892\u20136903. IEEE, 2024.\nKarl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn,\nand Sergey Levine. Fast: Efficient action tokenization for vision-language-action models. arXiv preprint\narXiv:2501.09747 , 2025.\nDelin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, JiaYuan Gu, Bin Zhao,\nDong Wang, et al. Spatialvla: Exploring spatial representations for visual-language-action model. arXiv\npreprint arXiv:2501.15830 , 2025.\nOcto Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey\nHejna, Tobias Kreiman, Charles Xu, et al. Octo: An open-source generalist robot policy. arXiv preprint\narXiv:2405.12213 , 2024.\nZachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In European conference\non computer vision , pages 402\u2013419. Springer, 2020.\nHomer Rich Walke, Kevin Black, Tony Z Zhao, Quan Vuong, Chongyi Zheng, Philippe Hansen-Estruch,\nAndre Wang He, Vivek Myers, Moo Jin Kim, Max Du, et al. Bridgedata v2: A dataset for robot learning at\nscale. In Conference on Robot Learning , pages 1723\u20131736. PMLR, 2023.\nXinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze\nWang, Zhen Li, Qiying Yu, Yingli Zhao, Yulong Ao, Xuebin Min, Tao Li, Boya Wu, Bo Zhao, Bowen Zhang,\nLiangdong Wang, Guang Liu, Zheqi He, Xi Yang, Jingjing Liu, Yonghua Lin, Tiejun Huang, and Zhongyuan\nWang. Emu3: Next-tokenpredictionisallyouneed, 2024. URL https://arxiv.org/abs/2409.18869 .\nYuqi Wang, Xinghang Li, Wenxuan Wang, Junbo Zhang, Yingyan Li, Yuntao Chen, Xinlong Wang, and\nZhaoxiang Zhang. Unified vision-language-action model, 2025. URL https://arxiv.org/abs/2506.\n19850.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\nChain-of-thought prompting elicits reasoning in large language models. Advances in neural information\nprocessing systems , 35:24824\u201324837, 2022.\nHongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu, Xinghang Li, Minghuan Liu, Hang Li,\nand Tao Kong. Unleashing large-scale video generative pre-training for visual robot manipulation. arXiv\npreprint arXiv:2312.13139 , 2023.\n14\n\n--- Page 15 ---\nFlowVLA: Thinking in Motion with a Visual Chain of Thought\nWentao Yuan, Jiafei Duan, Valts Blukis, Wilbert Pumacay, Ranjay Krishna, Adithyavairavan Murali, Arsalan\nMousavian, and Dieter Fox. Robopoint: A vision-language model for spatial affordance prediction for\nrobotics. arXiv preprint arXiv:2406.10721 , 2024.\nYifu Yuan, Haiqin Cui, Yibin Chen, Zibin Dong, Fei Ni, Longxin Kou, Jinyi Liu, Pengyi Li, Yan Zheng, and\nJianye Hao. From seeing to doing: Bridging reasoning and decision for robotic manipulation. arXiv preprint\narXiv:2505.08548 , 2025a.\nYifu Yuan, Haiqin Cui, Yaoting Huang, Yibin Chen, Fei Ni, Zibin Dong, Pengyi Li, Yan Zheng, and Jianye\nHao. Embodied-r1: Reinforced embodied reasoning for general robotic manipulation, 2025b. URL\nhttps://arxiv.org/abs/2508.13998 .\nMicha\u0142 Zawalski, William Chen, Karl Pertsch, Oier Mees, Chelsea Finn, and Sergey Levine. Robotic control\nvia embodied chain-of-thought reasoning. arXiv preprint arXiv:2407.08693 , 2024.\nJia Zeng, Qingwen Bu, Bangjun Wang, Wenke Xia, Li Chen, Hao Dong, Haoming Song, Dong Wang, Di Hu,\nPing Luo, et al. Learning manipulation by predicting interaction. arXiv preprint arXiv:2406.00439 , 2024.\nQingqing Zhao, Yao Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang Zhang, Yecheng Wu, Zhaoshuo Li, Qianli Ma,\nSong Han, Chelsea Finn, et al. Cot-vla: Visual chain-of-thought reasoning for vision-language-action\nmodels. In Proceedings of the Computer Vision and Pattern Recognition Conference , pages 1702\u20131713, 2025.\nRuijie Zheng, Yongyuan Liang, Shuaiyi Huang, Jianfeng Gao, Hal Daum\u00e9 III, Andrey Kolobov, Furong Huang,\nand Jianwei Yang. Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist\nrobotic policies. arXiv preprint arXiv:2412.10345 , 2024.\nBrianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker,\nAyzaan Wahid, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In\nConference on Robot Learning , pages 2165\u20132183. PMLR, 2023.\n15",
  "project_dir": "artifacts/projects/FlowVLA_Visual_Chain_of_Thought_Robot_World_Model",
  "communication_dir": "artifacts/projects/FlowVLA_Visual_Chain_of_Thought_Robot_World_Model/.agent_comm",
  "assigned_at": "2025-08-26T20:54:08.361064",
  "status": "assigned"
}